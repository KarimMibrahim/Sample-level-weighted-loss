{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Needed to install pyprotocols, which needed 'git clone https://github.com/pdollar/coco.git'\n",
    "to be cloned and 'make' the pythonapi, which needed 'make' to be installed, which needed 'apt-get update'\n",
    "check https://github.com/cocodataset/cocoapi/issues/8\n",
    "and https://askubuntu.com/questions/192645/make-command-not-found\n",
    "\"\"\"\n",
    "from gluoncv import data, utils\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import random \n",
    "from skimage.transform import resize\n",
    "\n",
    "random.seed = 11\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=16.66s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.44s)\n",
      "creating index...\n",
      "index created!\n",
      "Num of training images: 117266\n",
      "Num of validation images: 4952\n"
     ]
    }
   ],
   "source": [
    "# Running the tutorial on mscoco\n",
    "train_dataset = data.COCODetection(splits=['instances_train2017'])\n",
    "val_dataset = data.COCODetection(splits=['instances_val2017'])\n",
    "print('Num of training images:', len(train_dataset))\n",
    "print('Num of validation images:', len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only presence labels [exclude image segementation information]\n",
    "labels_list = [labels[1][:,4:5].ravel() for labels in train_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to binary labels format\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "binarized_labels = mlb.fit_transform(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format in pandas dataframe with columns as class names\n",
    "labels_names = train_dataset.index_map\n",
    "import operator\n",
    "sorted_labels_names = sorted(labels_names.items(), key=operator.itemgetter(1))\n",
    "sorted_names = [label_name[0] for label_name in sorted_labels_names]\n",
    "labels_df = pd.DataFrame(binarized_labels,columns=sorted_names)\n",
    "#labels_df.to_csv('Datasets/binarized_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "28000\n",
      "28500\n",
      "29000\n",
      "29500\n",
      "30000\n",
      "30500\n",
      "31000\n",
      "31500\n",
      "32000\n",
      "32500\n",
      "33000\n",
      "33500\n",
      "34000\n",
      "34500\n",
      "35000\n",
      "35500\n",
      "36000\n",
      "36500\n",
      "37000\n",
      "37500\n",
      "38000\n",
      "38500\n",
      "39000\n",
      "39500\n",
      "40000\n",
      "40500\n",
      "41000\n",
      "41500\n",
      "42000\n",
      "42500\n",
      "43000\n",
      "43500\n",
      "44000\n",
      "44500\n",
      "45000\n",
      "45500\n",
      "46000\n",
      "46500\n",
      "47000\n",
      "47500\n",
      "48000\n",
      "48500\n",
      "49000\n",
      "49500\n",
      "50000\n",
      "50500\n",
      "51000\n",
      "51500\n",
      "52000\n",
      "52500\n",
      "53000\n",
      "53500\n",
      "54000\n",
      "54500\n",
      "55000\n",
      "55500\n",
      "56000\n",
      "56500\n",
      "57000\n",
      "57500\n",
      "58000\n",
      "58500\n",
      "59000\n",
      "59500\n",
      "60000\n",
      "60500\n",
      "61000\n",
      "61500\n",
      "62000\n",
      "62500\n",
      "63000\n",
      "63500\n",
      "64000\n",
      "64500\n",
      "65000\n",
      "65500\n",
      "66000\n",
      "66500\n",
      "67000\n",
      "67500\n",
      "68000\n",
      "68500\n",
      "69000\n",
      "69500\n",
      "70000\n",
      "70500\n",
      "71000\n",
      "71500\n",
      "72000\n",
      "72500\n",
      "73000\n",
      "73500\n",
      "74000\n",
      "74500\n",
      "75000\n",
      "75500\n",
      "76000\n",
      "76500\n",
      "77000\n",
      "77500\n",
      "78000\n",
      "78500\n",
      "79000\n",
      "79500\n",
      "80000\n",
      "80500\n",
      "81000\n",
      "81500\n",
      "82000\n",
      "82500\n",
      "83000\n",
      "83500\n",
      "84000\n",
      "84500\n",
      "85000\n",
      "85500\n",
      "86000\n",
      "86500\n",
      "87000\n",
      "87500\n",
      "88000\n",
      "88500\n",
      "89000\n",
      "89500\n",
      "90000\n",
      "90500\n",
      "91000\n",
      "91500\n",
      "92000\n",
      "92500\n",
      "93000\n",
      "93500\n",
      "94000\n",
      "94500\n",
      "95000\n",
      "95500\n",
      "96000\n",
      "96500\n",
      "97000\n",
      "97500\n",
      "98000\n",
      "98500\n",
      "99000\n",
      "99500\n",
      "100000\n",
      "100500\n",
      "101000\n",
      "101500\n",
      "102000\n",
      "102500\n",
      "106500\n",
      "107000\n",
      "107500\n",
      "108000\n",
      "108500\n",
      "109000\n",
      "109500\n",
      "110000\n",
      "110500\n",
      "111000\n",
      "111500\n",
      "112000\n",
      "112500\n",
      "113000\n",
      "113500\n",
      "114000\n",
      "114500\n",
      "115000\n",
      "115500\n",
      "116000\n",
      "116500\n",
      "117000\n"
     ]
    }
   ],
   "source": [
    "INPUT_SHAPE = (224, 224, 3)\n",
    "INPUT_IMAGE_MEAN = [0.485, 0.456, 0.406]\n",
    "INPUT_IMAGE_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "counter = 0\n",
    "for image, label in train_dataset:\n",
    "    image=image.asnumpy()\n",
    "    image = image.astype(np.float32)\n",
    "    image /= 255.0\n",
    "    image = (image - INPUT_IMAGE_MEAN) / INPUT_IMAGE_STD\n",
    "    image = resize(image, INPUT_SHAPE)\n",
    "    image = image.astype(np.float32)\n",
    "    np.savez(\"/srv/workspace/research/mlml/datasets/mscoco/train_formatted_normalized_npz/\"+str(counter), image = image)\n",
    "    counter += 1\n",
    "    if counter%500 == 0:\n",
    "        print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.reset_index(level=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.to_csv('/srv/workspace/research/mlml/datasets/mscoco/binarized_labels_normalized.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.load(os.path.join(\"/srv/workspace/research/mlml/datasets/mscoco/train_formatted_normalized_npz/\", str('50848') + '.npz'))['image']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format the image to match the model [MAYBE DO THAT BEFORE SAVING?]\n",
    "transform_train = transforms.Compose([\n",
    "    transforms.Resize(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
    "])\n",
    "img = transform_test(train_dataset[0][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract only presence labels [exclude image segementation information]\n",
    "labels_list =[labels[1][:,4:5].ravel() for labels in val_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to binary labels format\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "mlb = MultiLabelBinarizer()\n",
    "binarized_labels = mlb.fit_transform(labels_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# format in pandas dataframe with columns as class names\n",
    "labels_names = val_dataset.index_map\n",
    "import operator\n",
    "sorted_labels_names = sorted(labels_names.items(), key=operator.itemgetter(1))\n",
    "sorted_names = [label_name[0] for label_name in sorted_labels_names]\n",
    "labels_df = pd.DataFrame(binarized_labels,columns=sorted_names)\n",
    "#labels_df.to_csv('Datasets/binarized_labels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "for image, label in val_dataset:\n",
    "    np.savez(\"/srv/workspace/research/mlml/datasets/mscoco/val_formatted_npz/\"+str(counter), image=image.asnumpy())\n",
    "    counter += 1\n",
    "    if counter%500 == 0:\n",
    "        print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.reset_index(level=0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_df.to_csv('/srv/workspace/research/mlml/datasets/mscoco/ms_val_binarized_labels.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create artifical missing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hide_labels(train_labels,ratio_of_hidden_samples = 0.4):\n",
    "    ones_indices = np.nonzero(train_labels)\n",
    "    number_of_hidden_samples = int(len(ones_indices[0]) * ratio_of_hidden_samples)\n",
    "    random_indices = random.sample(list(np.arange(len(ones_indices[0]))),number_of_hidden_samples)\n",
    "    indices_to_hide = (ones_indices[0][random_indices] , ones_indices[1][random_indices])\n",
    "    labels_with_missing_positives = np.copy(train_labels)\n",
    "    for counter in range(number_of_hidden_samples):\n",
    "        labels_with_missing_positives[indices_to_hide[0][counter]][indices_to_hide[1][counter]] = 0\n",
    "    return labels_with_missing_positives, indices_to_hide\n",
    "\n",
    "def get_weights_for_hidden_labels(train_labels,indices_to_hide, pos_weights = 1):\n",
    "    train_negative_weights = np.zeros_like(train_labels) + 1 \n",
    "    train_positive_weights = np.zeros_like(train_labels) + pos_weights # We make positive weight 5 becuase of data imbalance\n",
    "    for counter in range (len(indices_to_hide[0])):\n",
    "        train_negative_weights[indices_to_hide[0][counter]][indices_to_hide[1][counter]] = 0\n",
    "    return train_negative_weights, train_positive_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIRECTORY = '/srv/workspace/research/mlml/datasets/mscoco/'\n",
    "global_labels = pd.read_csv('/srv/workspace/research/mlml/datasets/mscoco//ms binarized_labels.csv')\n",
    "labels_array = global_labels.drop([\"Unnamed: 0\", \"index\"],axis=1).values\n",
    "ratios_to_hide = np.arange(0,1,0.1)\n",
    "for ratio in ratios_to_hide:\n",
    "    labels_with_missing_positives, indices_to_hide = hide_labels(labels_array,ratio)\n",
    "    labels_with_missing_positives_df = global_labels.copy()\n",
    "    labels_with_missing_positives_df.iloc[:,2:]=labels_with_missing_positives\n",
    "    labels_with_missing_positives_df.to_csv(os.path.join(SAVE_DIRECTORY,'missing_labels'+str(ratio)+'.csv'))\n",
    "    \n",
    "    train_negative_weights, train_positive_weights = get_weights_for_hidden_labels(labels_with_missing_positives,\n",
    "                                                                                  indices_to_hide)\n",
    "    train_negative_weights_df = global_labels.copy()\n",
    "    train_negative_weights_df.iloc[:,2:]=train_negative_weights\n",
    "    train_negative_weights_df.to_csv(os.path.join(SAVE_DIRECTORY,'negative_weights'+str(ratio)+'.csv'))\n",
    "    \n",
    "    train_positive_weights_df = global_labels.copy()\n",
    "    train_positive_weights_df.iloc[:,2:]=train_positive_weights\n",
    "    train_positive_weights_df.to_csv(os.path.join(SAVE_DIRECTORY,'positive_weights'+str(ratio)+'.csv'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating train test splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skmultilearn.model_selection import iterative_train_test_split\n",
    "#NUM_SPLITS = 4\n",
    "global_labels = pd.read_csv('/srv/workspace/research/mlml/datasets/mscoco//ms binarized_labels.csv')\n",
    "labels_array = global_labels.drop([\"Unnamed: 0\", \"index\"],axis=1).values\n",
    "train_indices = global_labels.iloc[:,0:2].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two splits\n",
    "X_half1, y_half1, X_half2, y_half2 = iterative_train_test_split(train_indices, labels_array, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split each split to two splits\n",
    "X_quarter1, y_quarter1, X_quarter2, y_quarter2 = iterative_train_test_split(X_half1, y_half1, test_size = 0.5)\n",
    "X_quarter3, y_quarter3, X_quarter4, y_quarter4 = iterative_train_test_split(X_half1, y_half1, test_size = 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0008515656484833"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ratio is almost equal between splits\n",
    "np.mean((np.sum(y_quarter1,axis = 0)/len(y_quarter1)) / (np.sum(y_quarter4,axis = 0)/len(y_quarter1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating ratios and splits at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hide_labels_per_sample(train_labels,ratio_of_hidden_samples = 0.4):\n",
    "    number_of_labels_to_hide_per_sample = np.round(np.sum(train_labels,axis=1)*ratio_of_hidden_samples)\n",
    "    train_negative_weights = np.zeros_like(train_labels) + 1 \n",
    "    labels_with_missing_positives = np.copy(train_labels)\n",
    "    for idx, sample in enumerate(labels_array):\n",
    "        indices_to_hide = random.sample(list(np.nonzero(sample)[0]),int(number_of_labels_to_hide_per_sample[idx]))\n",
    "        labels_with_missing_positives[idx][indices_to_hide] = 0 \n",
    "        train_negative_weights[idx][indices_to_hide] = 0 \n",
    "    return labels_with_missing_positives, train_negative_weights\n",
    "\n",
    "def get_positive_weights(train_labels, pos_weights = 1):\n",
    "    train_positive_weights = np.zeros_like(train_labels) + pos_weights # We make positive weight 5 becuase of data imbalance\n",
    "    return train_positive_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAVE_DIRECTORY = '/srv/workspace/research/mlml/mlml_weightedLoss/labels_balanced/'\n",
    "global_labels = pd.read_csv('/srv/workspace/research/mlml/datasets/mscoco/l.csv')\n",
    "global_labels.drop([\"Unnamed: 0\", \"index\"],axis=1, inplace= True)\n",
    "labels_array = global_labels.values\n",
    "ratios_to_hide = np.arange(0,1,0.2)\n",
    "for ratio in ratios_to_hide:\n",
    "    ratio_save_dir = os.path.join(SAVE_DIRECTORY,'missing_labels'+str(round(ratio, 1)))\n",
    "    os.makedirs(ratio_save_dir,exist_ok=True)\n",
    "    labels_with_missing_positives, train_negative_weights = hide_labels_per_sample(labels_array,ratio)\n",
    "    labels_with_missing_positives_df = global_labels.copy() \n",
    "    labels_with_missing_positives_df.iloc[:,:] = labels_with_missing_positives\n",
    "    labels_with_missing_positives_df.to_csv(os.path.join(ratio_save_dir,\n",
    "                                                        'missing_labels'+str(round(ratio, 1))+'.csv'),\n",
    "                                                        index = False)\n",
    "    \n",
    "    train_negative_weights_df = global_labels.copy()\n",
    "    train_negative_weights_df.iloc[:,:]=train_negative_weights\n",
    "    train_negative_weights_df.to_csv(os.path.join(ratio_save_dir,'negative_weights'+str(round(ratio, 1))+'.csv'),\n",
    "                                                        index = False)\n",
    "    \n",
    "    train_positive_weights = get_positive_weights(labels_with_missing_positives)\n",
    "    train_positive_weights_df = global_labels.copy()\n",
    "    train_positive_weights_df.iloc[:,:]=train_positive_weights\n",
    "    train_positive_weights_df.to_csv(os.path.join(ratio_save_dir,'positive_weights'+str(round(ratio, 1))+'.csv'),\n",
    "                                                        index = False)\n",
    "                                  \n",
    "    #making splits\n",
    "    train_indices = np.asarray([global_labels.index.values]*2).T # duplicating the column because iterative splits expects >1 dim\n",
    "    # create two splits\n",
    "    X_half1, y_half1, X_half2, y_half2 = iterative_train_test_split(train_indices, labels_with_missing_positives, test_size = 0.5)\n",
    "    # split each split to two splits\n",
    "    X_quarter1, y_quarter1, X_quarter2, y_quarter2 = iterative_train_test_split(X_half1, y_half1, test_size = 0.5)\n",
    "    X_quarter3, y_quarter3, X_quarter4, y_quarter4 = iterative_train_test_split(X_half2, y_half2, test_size = 0.5)      \n",
    "    split1 = pd.DataFrame(y_quarter1, columns=global_labels.columns, index = X_quarter1[:,0])\n",
    "    split2 = pd.DataFrame(y_quarter2, columns=global_labels.columns, index = X_quarter2[:,0])\n",
    "    split3 = pd.DataFrame(y_quarter3, columns=global_labels.columns, index = X_quarter3[:,0])\n",
    "    split4 = pd.DataFrame(y_quarter4, columns=global_labels.columns, index = X_quarter4[:,0])\n",
    "    train1 = pd.concat([split1,split2,split3])\n",
    "    test1 = split4\n",
    "    train2 = pd.concat([split1,split2,split4])\n",
    "    test2 = split3\n",
    "    train3 = pd.concat([split1,split3,split4])\n",
    "    test3 = split2\n",
    "    train4 = pd.concat([split2,split3,split4])\n",
    "    test4 = split1\n",
    "    \n",
    "    train1.to_csv(os.path.join(ratio_save_dir,'train1_'+str(round(ratio, 1))+'.csv'))  \n",
    "    train2.to_csv(os.path.join(ratio_save_dir,'train2_'+str(round(ratio, 1))+'.csv'))  \n",
    "    train3.to_csv(os.path.join(ratio_save_dir,'train3_'+str(round(ratio, 1))+'.csv'))  \n",
    "    train4.to_csv(os.path.join(ratio_save_dir,'train4_'+str(round(ratio, 1))+'.csv')) \n",
    "    \n",
    "    test1.to_csv(os.path.join(ratio_save_dir,'test1_'+str(round(ratio, 1))+'.csv'))  \n",
    "    test2.to_csv(os.path.join(ratio_save_dir,'test2_'+str(round(ratio, 1))+'.csv'))  \n",
    "    test3.to_csv(os.path.join(ratio_save_dir,'test3_'+str(round(ratio, 1))+'.csv'))  \n",
    "    test4.to_csv(os.path.join(ratio_save_dir,'test4_'+str(round(ratio, 1))+'.csv'))                                                               "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## make splits, with complete testsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "SAVE_DIRECTORY = '/srv/workspace/research/mlml/mlml_weightedLoss/labels_balanced/'\n",
    "global_labels = pd.read_csv('/srv/workspace/research/mlml/datasets/mscoco/binarized_labels_normalized.csv')\n",
    "global_labels.drop([\"Unnamed: 0\", \"index\"],axis=1, inplace= True)\n",
    "labels_array = global_labels.values\n",
    "\n",
    "#making splits\n",
    "train_indices = np.asarray([global_labels.index.values]*2).T # duplicating the column because iterative splits expects >1 dim\n",
    "# create two splits\n",
    "X_half1, y_half1, X_half2, y_half2 = iterative_train_test_split(train_indices, labels_array, test_size = 0.5)\n",
    "# split each split to two splits\n",
    "X_quarter1, y_quarter1, X_quarter2, y_quarter2 = iterative_train_test_split(X_half1, y_half1, test_size = 0.5)\n",
    "X_quarter3, y_quarter3, X_quarter4, y_quarter4 = iterative_train_test_split(X_half2, y_half2, test_size = 0.5)      \n",
    "split1 = pd.DataFrame(y_quarter1, columns=global_labels.columns, index = X_quarter1[:,0])\n",
    "split2 = pd.DataFrame(y_quarter2, columns=global_labels.columns, index = X_quarter2[:,0])\n",
    "split3 = pd.DataFrame(y_quarter3, columns=global_labels.columns, index = X_quarter3[:,0])\n",
    "split4 = pd.DataFrame(y_quarter4, columns=global_labels.columns, index = X_quarter4[:,0])\n",
    "train1 = pd.concat([split1,split2,split3])\n",
    "test1 = split4\n",
    "train2 = pd.concat([split1,split2,split4])\n",
    "test2 = split3\n",
    "train3 = pd.concat([split1,split3,split4])\n",
    "test3 = split2\n",
    "train4 = pd.concat([split2,split3,split4])\n",
    "test4 = split1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "ratios_to_hide = np.arange(0,1,0.2)\n",
    "for ratio in ratios_to_hide:\n",
    "    ratio_save_dir = os.path.join(SAVE_DIRECTORY,'missing_labels'+str(round(ratio, 1)))\n",
    "    os.makedirs(ratio_save_dir,exist_ok=True)\n",
    "    # Split1\n",
    "    labels_array = train1.values\n",
    "    labels_with_missing_positives, train_negative_weights = hide_labels_per_sample(labels_array,ratio)\n",
    "    labels_with_missing_positives_df = train1.copy() \n",
    "    labels_with_missing_positives_df.iloc[:,:] = labels_with_missing_positives\n",
    "    labels_with_missing_positives_df.to_csv(os.path.join(ratio_save_dir,\n",
    "                                                        'train1_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_negative_weights_df = train1.copy()\n",
    "    train_negative_weights_df.iloc[:,:]=train_negative_weights\n",
    "    train_negative_weights_df.to_csv(os.path.join(ratio_save_dir,'negative_weights1_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_positive_weights = get_positive_weights(labels_with_missing_positives)\n",
    "    train_positive_weights_df = train1.copy()\n",
    "    train_positive_weights_df.iloc[:,:]=train_positive_weights\n",
    "    train_positive_weights_df.to_csv(os.path.join(ratio_save_dir,'positive_weights'+str(round(ratio, 1))+'.csv'),\n",
    "                                                        index = False)\n",
    "    \n",
    "    test1.to_csv(os.path.join(ratio_save_dir,'test1_'+str(round(ratio, 1))+'.csv'))  \n",
    "    \n",
    "    \n",
    "    # Split2\n",
    "    labels_array = train2.values\n",
    "    labels_with_missing_positives, train_negative_weights = hide_labels_per_sample(labels_array,ratio)\n",
    "    labels_with_missing_positives_df = train2.copy() \n",
    "    labels_with_missing_positives_df.iloc[:,:] = labels_with_missing_positives\n",
    "    labels_with_missing_positives_df.to_csv(os.path.join(ratio_save_dir,\n",
    "                                                        'train2_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_negative_weights_df = train2.copy()\n",
    "    train_negative_weights_df.iloc[:,:]=train_negative_weights\n",
    "    train_negative_weights_df.to_csv(os.path.join(ratio_save_dir,'negative_weights2_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_positive_weights = get_positive_weights(labels_with_missing_positives)\n",
    "    train_positive_weights_df = train2.copy()\n",
    "    train_positive_weights_df.iloc[:,:]=train_positive_weights\n",
    "    train_positive_weights_df.to_csv(os.path.join(ratio_save_dir,'positive_weights'+str(round(ratio, 1))+'.csv'),\n",
    "                                                        index = False)\n",
    "    \n",
    "    test2.to_csv(os.path.join(ratio_save_dir,'test2_'+str(round(ratio, 1))+'.csv'))  \n",
    "    \n",
    "    # Split3\n",
    "    labels_array = train3.values\n",
    "    labels_with_missing_positives, train_negative_weights = hide_labels_per_sample(labels_array,ratio)\n",
    "    labels_with_missing_positives_df = train3.copy() \n",
    "    labels_with_missing_positives_df.iloc[:,:] = labels_with_missing_positives\n",
    "    labels_with_missing_positives_df.to_csv(os.path.join(ratio_save_dir,\n",
    "                                                        'train3_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_negative_weights_df = train3.copy()\n",
    "    train_negative_weights_df.iloc[:,:]=train_negative_weights\n",
    "    train_negative_weights_df.to_csv(os.path.join(ratio_save_dir,'negative_weights3_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_positive_weights = get_positive_weights(labels_with_missing_positives)\n",
    "    train_positive_weights_df = train3.copy()\n",
    "    train_positive_weights_df.iloc[:,:]=train_positive_weights\n",
    "    train_positive_weights_df.to_csv(os.path.join(ratio_save_dir,'positive_weights'+str(round(ratio, 1))+'.csv'),\n",
    "                                                        index = False)\n",
    "    \n",
    "    test3.to_csv(os.path.join(ratio_save_dir,'test3_'+str(round(ratio, 1))+'.csv'))  \n",
    "\n",
    "                                  \n",
    "    # Split4\n",
    "    labels_array = train4.values\n",
    "    labels_with_missing_positives, train_negative_weights = hide_labels_per_sample(labels_array,ratio)\n",
    "    labels_with_missing_positives_df = train4.copy() \n",
    "    labels_with_missing_positives_df.iloc[:,:] = labels_with_missing_positives\n",
    "    labels_with_missing_positives_df.to_csv(os.path.join(ratio_save_dir,\n",
    "                                                        'train4_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_negative_weights_df = train4.copy()\n",
    "    train_negative_weights_df.iloc[:,:]=train_negative_weights\n",
    "    train_negative_weights_df.to_csv(os.path.join(ratio_save_dir,'negative_weights4_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_positive_weights = get_positive_weights(labels_with_missing_positives)\n",
    "    train_positive_weights_df = train4.copy()\n",
    "    train_positive_weights_df.iloc[:,:]=train_positive_weights\n",
    "    train_positive_weights_df.to_csv(os.path.join(ratio_save_dir,'positive_weights'+str(round(ratio, 1))+'.csv'),\n",
    "                                                        index = False)\n",
    "    \n",
    "    test4.to_csv(os.path.join(ratio_save_dir,'test4_'+str(round(ratio, 1))+'.csv'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Work only on images with 4 or more labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "SAVE_DIRECTORY = '/srv/workspace/research/mlml/mlml_weightedLoss/labels_balanced_4labels/'\n",
    "global_labels = pd.read_csv('/srv/workspace/research/mlml/datasets/mscoco/binarized_labels_normalized.csv')\n",
    "global_labels.drop([\"Unnamed: 0\", \"index\"],axis=1, inplace= True)\n",
    "\n",
    "#select only images with more than 4 labels\n",
    "global_labels = global_labels[global_labels.sum(axis=1) >= 4]\n",
    "labels_array = global_labels.values\n",
    "\n",
    "#making splits\n",
    "train_indices = np.asarray([global_labels.index.values]*2).T # duplicating the column because iterative splits expects >1 dim\n",
    "# create two splits\n",
    "X_half1, y_half1, X_half2, y_half2 = iterative_train_test_split(train_indices, labels_array, test_size = 0.5)\n",
    "# split each split to two splits\n",
    "X_quarter1, y_quarter1, X_quarter2, y_quarter2 = iterative_train_test_split(X_half1, y_half1, test_size = 0.5)\n",
    "X_quarter3, y_quarter3, X_quarter4, y_quarter4 = iterative_train_test_split(X_half2, y_half2, test_size = 0.5)      \n",
    "split1 = pd.DataFrame(y_quarter1, columns=global_labels.columns, index = X_quarter1[:,0])\n",
    "split2 = pd.DataFrame(y_quarter2, columns=global_labels.columns, index = X_quarter2[:,0])\n",
    "split3 = pd.DataFrame(y_quarter3, columns=global_labels.columns, index = X_quarter3[:,0])\n",
    "split4 = pd.DataFrame(y_quarter4, columns=global_labels.columns, index = X_quarter4[:,0])\n",
    "train1 = pd.concat([split1,split2,split3])\n",
    "test1 = split4\n",
    "train2 = pd.concat([split1,split2,split4])\n",
    "test2 = split3\n",
    "train3 = pd.concat([split1,split3,split4])\n",
    "test3 = split2\n",
    "train4 = pd.concat([split2,split3,split4])\n",
    "test4 = split1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "ratios_to_hide = np.arange(0,1,0.25)\n",
    "for ratio in ratios_to_hide:\n",
    "    ratio_save_dir = os.path.join(SAVE_DIRECTORY,'missing_labels'+str(round(ratio, 1)))\n",
    "    os.makedirs(ratio_save_dir,exist_ok=True)\n",
    "    # Split1\n",
    "    labels_array = train1.values\n",
    "    labels_with_missing_positives, train_negative_weights = hide_labels_per_sample(labels_array,ratio)\n",
    "    labels_with_missing_positives_df = train1.copy() \n",
    "    labels_with_missing_positives_df.iloc[:,:] = labels_with_missing_positives\n",
    "    labels_with_missing_positives_df.to_csv(os.path.join(ratio_save_dir,\n",
    "                                                        'train1_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_negative_weights_df = train1.copy()\n",
    "    train_negative_weights_df.iloc[:,:]=train_negative_weights\n",
    "    train_negative_weights_df.to_csv(os.path.join(ratio_save_dir,'negative_weights1_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_positive_weights = get_positive_weights(labels_with_missing_positives)\n",
    "    train_positive_weights_df = train1.copy()\n",
    "    train_positive_weights_df.iloc[:,:]=train_positive_weights\n",
    "    train_positive_weights_df.to_csv(os.path.join(ratio_save_dir,'positive_weights'+str(round(ratio, 1))+'.csv'),\n",
    "                                                        index = False)\n",
    "    \n",
    "    test1.to_csv(os.path.join(ratio_save_dir,'test1_'+str(round(ratio, 1))+'.csv'))  \n",
    "    \n",
    "    \n",
    "    # Split2\n",
    "    labels_array = train2.values\n",
    "    labels_with_missing_positives, train_negative_weights = hide_labels_per_sample(labels_array,ratio)\n",
    "    labels_with_missing_positives_df = train2.copy() \n",
    "    labels_with_missing_positives_df.iloc[:,:] = labels_with_missing_positives\n",
    "    labels_with_missing_positives_df.to_csv(os.path.join(ratio_save_dir,\n",
    "                                                        'train2_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_negative_weights_df = train2.copy()\n",
    "    train_negative_weights_df.iloc[:,:]=train_negative_weights\n",
    "    train_negative_weights_df.to_csv(os.path.join(ratio_save_dir,'negative_weights2_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_positive_weights = get_positive_weights(labels_with_missing_positives)\n",
    "    train_positive_weights_df = train2.copy()\n",
    "    train_positive_weights_df.iloc[:,:]=train_positive_weights\n",
    "    train_positive_weights_df.to_csv(os.path.join(ratio_save_dir,'positive_weights'+str(round(ratio, 1))+'.csv'),\n",
    "                                                        index = False)\n",
    "    \n",
    "    test2.to_csv(os.path.join(ratio_save_dir,'test2_'+str(round(ratio, 1))+'.csv'))  \n",
    "    \n",
    "    # Split3\n",
    "    labels_array = train3.values\n",
    "    labels_with_missing_positives, train_negative_weights = hide_labels_per_sample(labels_array,ratio)\n",
    "    labels_with_missing_positives_df = train3.copy() \n",
    "    labels_with_missing_positives_df.iloc[:,:] = labels_with_missing_positives\n",
    "    labels_with_missing_positives_df.to_csv(os.path.join(ratio_save_dir,\n",
    "                                                        'train3_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_negative_weights_df = train3.copy()\n",
    "    train_negative_weights_df.iloc[:,:]=train_negative_weights\n",
    "    train_negative_weights_df.to_csv(os.path.join(ratio_save_dir,'negative_weights3_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_positive_weights = get_positive_weights(labels_with_missing_positives)\n",
    "    train_positive_weights_df = train3.copy()\n",
    "    train_positive_weights_df.iloc[:,:]=train_positive_weights\n",
    "    train_positive_weights_df.to_csv(os.path.join(ratio_save_dir,'positive_weights'+str(round(ratio, 1))+'.csv'),\n",
    "                                                        index = False)\n",
    "    \n",
    "    test3.to_csv(os.path.join(ratio_save_dir,'test3_'+str(round(ratio, 1))+'.csv'))  \n",
    "\n",
    "                                  \n",
    "    # Split4\n",
    "    labels_array = train4.values\n",
    "    labels_with_missing_positives, train_negative_weights = hide_labels_per_sample(labels_array,ratio)\n",
    "    labels_with_missing_positives_df = train4.copy() \n",
    "    labels_with_missing_positives_df.iloc[:,:] = labels_with_missing_positives\n",
    "    labels_with_missing_positives_df.to_csv(os.path.join(ratio_save_dir,\n",
    "                                                        'train4_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_negative_weights_df = train4.copy()\n",
    "    train_negative_weights_df.iloc[:,:]=train_negative_weights\n",
    "    train_negative_weights_df.to_csv(os.path.join(ratio_save_dir,'negative_weights4_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_positive_weights = get_positive_weights(labels_with_missing_positives)\n",
    "    train_positive_weights_df = train4.copy()\n",
    "    train_positive_weights_df.iloc[:,:]=train_positive_weights\n",
    "    train_positive_weights_df.to_csv(os.path.join(ratio_save_dir,'positive_weights'+str(round(ratio, 1))+'.csv'),\n",
    "                                                        index = False)\n",
    "    \n",
    "    test4.to_csv(os.path.join(ratio_save_dir,'test4_'+str(round(ratio, 1))+'.csv'))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "missing = pd.read_csv('/srv/workspace/research/mlml/mlml_weightedLoss/labels_balanced_4labels/missing_labels'+\n",
    "                            str(round(0.75, 1))+'/train' + str(1) +\"_\" + str(round(0.75, 1)) + '.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_labels = pd.read_csv('/srv/workspace/research/mlml/mlml_weightedLoss/labels_balanced_4labels/missing_labels'+\n",
    "                            str(round(0.0, 1))+'/train' + str(1) +\"_\" + str(round(0.0, 1)) + '.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating correlation weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "code_folding": [
     0,
     21
    ]
   },
   "outputs": [],
   "source": [
    "def negative_labeles_probabilities(hot_encoded):\n",
    "    # count the number of times a combination has appeared with the negative label as 1 / the total number of\n",
    "    # occurances of that combination without the negative label\n",
    "    negative_weights = np.zeros([len(hot_encoded), len(LABELS_LIST)])\n",
    "    for sample_idx in range(len(hot_encoded)):\n",
    "        for label_idx in range(len(LABELS_LIST)):\n",
    "            if hot_encoded.iloc[sample_idx, label_idx+1] == 1:\n",
    "                negative_weights[sample_idx, label_idx] = 0\n",
    "            else:\n",
    "                temp_combination = hot_encoded.iloc[sample_idx,1:].copy()\n",
    "                temp_combination[label_idx] = 1\n",
    "                positive_samples = len(hot_encoded[(hot_encoded.iloc[:, 1:].values == temp_combination.values).all(axis = 1)])\n",
    "                negative_samples = len(hot_encoded[(hot_encoded.iloc[:, 1:].values == hot_encoded.iloc[sample_idx, 1:].values).all(axis=1)])\n",
    "                negative_weights[sample_idx, label_idx] = negative_samples / (positive_samples + negative_samples)\n",
    "    negative_weights_df = pd.DataFrame(negative_weights, columns=LABELS_LIST)\n",
    "    negative_weights_df[\"song_id\"] = hot_encoded.song_id\n",
    "    negative_weights_df = negative_weights_df[[\"song_id\"] + LABELS_LIST]\n",
    "    #negative_weights_df.to_csv(\"/home/karim/Documents/BalancedDatasetDeezer/GroundTruth/negative_weights.csv\",index=False)\n",
    "    return negative_weights_df\n",
    "\n",
    "\n",
    "def negative_labeles_probabilities_ignoring_zeros(hot_encoded):\n",
    "    # count the number of times a combination has appeared with the negative label as 1 / the total number of\n",
    "    # occurances of that combination without the negative label\n",
    "    negative_weights = np.zeros([len(hot_encoded), len(LABELS_LIST)])\n",
    "    for sample_idx in range(len(hot_encoded)):\n",
    "        for label_idx in range(len(LABELS_LIST)):\n",
    "            if hot_encoded.iloc[sample_idx, label_idx+1] == 1:\n",
    "                negative_weights[sample_idx, label_idx] = 0\n",
    "            else:\n",
    "                temp_combination = hot_encoded.iloc[sample_idx,1:].copy()\n",
    "                temp_combination[label_idx] = 1\n",
    "                # Compare only columns that are equal to 1, and count number of matches\n",
    "                # adding one to skip the song_id column, which exists in the hot_encoded dataframe\n",
    "                positive_columns = np.where(temp_combination.values == 1)[0] + 1\n",
    "                positive_samples = len(hot_encoded[(hot_encoded.iloc[:, positive_columns].values == 1).all(axis = 1)])\n",
    "                # Count occurances with the negative sample\n",
    "                temp_combination[label_idx] = 0\n",
    "                positive_columns = np.where(temp_combination.values == 1)[0] + 1\n",
    "                total_occurances_of_pattern = len(hot_encoded[(hot_encoded.iloc[:, positive_columns].values == 1).all(axis = 1)])\n",
    "                negative_weights[sample_idx, label_idx] = (total_occurances_of_pattern - positive_samples) / total_occurances_of_pattern\n",
    "    negative_weights_df = pd.DataFrame(negative_weights, columns=LABELS_LIST)\n",
    "    negative_weights_df[\"song_id\"] = hot_encoded.song_id\n",
    "    negative_weights_df = negative_weights_df[[\"song_id\"] + LABELS_LIST]\n",
    "    #negative_weights_df.to_csv(\"/home/karim/Documents/BalancedDatasetDeezer/GroundTruth/negative_weights.csv\",index=False)\n",
    "    return negative_weights_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-121-d2484809cc89>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnegative_labeles_probabilities_ignoring_zeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglobal_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-112-aaf5c7c86b06>\u001b[0m in \u001b[0;36mnegative_labeles_probabilities_ignoring_zeros\u001b[0;34m(hot_encoded)\u001b[0m\n\u001b[1;32m     39\u001b[0m                 \u001b[0mpositive_columns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_combination\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m                 \u001b[0mtotal_occurances_of_pattern\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhot_encoded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhot_encoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpositive_columns\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m                 \u001b[0mnegative_weights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtotal_occurances_of_pattern\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mpositive_samples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mtotal_occurances_of_pattern\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mnegative_weights_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnegative_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLABELS_LIST\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mnegative_weights_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"song_id\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhot_encoded\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msong_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "negative_labeles_probabilities_ignoring_zeros(global_labels.reset_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_weights = np.zeros([len(hot_encoded), len(LABELS_LIST)])\n",
    "for sample_idx in range(len(hot_encoded)):\n",
    "    for label_idx in range(len(LABELS_LIST)):\n",
    "        if hot_encoded.iloc[sample_idx, label_idx+1] == 1:\n",
    "            negative_weights[sample_idx, label_idx] = 0\n",
    "        else:\n",
    "            temp_combination = hot_encoded.iloc[sample_idx,1:].copy()\n",
    "            temp_combination[label_idx] = 1\n",
    "            # Compare only columns that are equal to 1, and count number of matches\n",
    "            # adding one to skip the song_id column, which exists in the hot_encoded dataframe\n",
    "            positive_columns = np.where(temp_combination.values == 1)[0] + 1\n",
    "            positive_samples = len(hot_encoded[(hot_encoded.iloc[:, positive_columns].values == 1).all(axis = 1)])\n",
    "            # Count occurances with the negative sample\n",
    "            temp_combination[label_idx] = 0\n",
    "            positive_columns = np.where(temp_combination.values == 1)[0] + 1\n",
    "            total_occurances_of_pattern = len(hot_encoded[(hot_encoded.iloc[:, positive_columns].values == 1).all(axis = 1)])\n",
    "            negative_weights[sample_idx, label_idx] = (total_occurances_of_pattern - positive_samples) / total_occurances_of_pattern\n",
    "negative_weights_df = pd.DataFrame(negative_weights, columns=LABELS_LIST)\n",
    "#negative_weights_df[\"song_id\"] = hot_encoded.song_id\n",
    "#negative_weights_df = negative_weights_df[[\"song_id\"] + LABELS_LIST]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_weights_df.index = global_labels.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_weights_df.to_csv(os.path.join('/srv/workspace/research/mlml/mlml_weightedLoss/labels_balanced_4labels/','negative_weights_global.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/srv/workspace/research/mlml/mlml_weightedLoss/labels_balanced_4labels/missing_labels0.8'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ratio_save_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
