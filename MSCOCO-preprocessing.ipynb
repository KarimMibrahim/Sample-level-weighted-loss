{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script to preprocess the mscoco dataset \n",
    "# We are using the Gluoncv library for interfacing with the MSCOCO dataset\n",
    "# Follow the instructions here on how to download the dataset: \n",
    "# https://gluon-cv.mxnet.io/build/examples_datasets/mscoco.html\n",
    "\n",
    "\"\"\"\n",
    "Needed to install pyprotocols, which needed 'git clone https://github.com/pdollar/coco.git'\n",
    "to be cloned and 'make' the pythonapi\n",
    "\"\"\"\n",
    "\n",
    "from gluoncv import data, utils\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "import random \n",
    "from skimage.transform import resize\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import operator\n",
    "\n",
    "random.seed = 11\n",
    "\n",
    "\n",
    "SAVING_PATH = \"/srv/workspace/research/mlml/datasets/mscoco/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=16.66s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.44s)\n",
      "creating index...\n",
      "index created!\n",
      "Num of training images: 117266\n",
      "Num of validation images: 4952\n"
     ]
    }
   ],
   "source": [
    "# Validating the access to the dataset\n",
    "train_dataset = data.COCODetection(splits=['instances_train2017'])\n",
    "val_dataset = data.COCODetection(splits=['instances_val2017'])\n",
    "print('Num of training images:', len(train_dataset))\n",
    "print('Num of validation images:', len(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract only classification labels [exclude image segementation information]\n",
    "labels_list = [labels[1][:,4:5].ravel() for labels in train_dataset]\n",
    "\n",
    "# Convert to binary labels format\n",
    "mlb = MultiLabelBinarizer()\n",
    "binarized_labels = mlb.fit_transform(labels_list)\n",
    "\n",
    "# format in pandas dataframe with columns as class names\n",
    "labels_names = train_dataset.index_map\n",
    "sorted_labels_names = sorted(labels_names.items(), key=operator.itemgetter(1))\n",
    "sorted_names = [label_name[0] for label_name in sorted_labels_names]\n",
    "labels_df = pd.DataFrame(binarized_labels,columns=sorted_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "1000\n",
      "1500\n",
      "2000\n",
      "2500\n",
      "3000\n",
      "3500\n",
      "4000\n",
      "4500\n",
      "5000\n",
      "5500\n",
      "6000\n",
      "6500\n",
      "7000\n",
      "7500\n",
      "8000\n",
      "8500\n",
      "9000\n",
      "9500\n",
      "10000\n",
      "10500\n",
      "11000\n",
      "11500\n",
      "12000\n",
      "12500\n",
      "13000\n",
      "13500\n",
      "14000\n",
      "14500\n",
      "15000\n",
      "15500\n",
      "16000\n",
      "16500\n",
      "17000\n",
      "17500\n",
      "18000\n",
      "18500\n",
      "19000\n",
      "19500\n",
      "20000\n",
      "20500\n",
      "21000\n",
      "21500\n",
      "22000\n",
      "22500\n",
      "23000\n",
      "23500\n",
      "24000\n",
      "24500\n",
      "25000\n",
      "25500\n",
      "26000\n",
      "26500\n",
      "27000\n",
      "27500\n",
      "28000\n",
      "28500\n",
      "29000\n",
      "29500\n",
      "30000\n",
      "30500\n",
      "31000\n",
      "31500\n",
      "32000\n",
      "32500\n",
      "33000\n",
      "33500\n",
      "34000\n",
      "34500\n",
      "35000\n",
      "35500\n",
      "36000\n",
      "36500\n",
      "37000\n",
      "37500\n",
      "38000\n",
      "38500\n",
      "39000\n",
      "39500\n",
      "40000\n",
      "40500\n",
      "41000\n",
      "41500\n",
      "42000\n",
      "42500\n",
      "43000\n",
      "43500\n",
      "44000\n",
      "44500\n",
      "45000\n",
      "45500\n",
      "46000\n",
      "46500\n",
      "47000\n",
      "47500\n",
      "48000\n",
      "48500\n",
      "49000\n",
      "49500\n",
      "50000\n",
      "50500\n",
      "51000\n",
      "51500\n",
      "52000\n",
      "52500\n",
      "53000\n",
      "53500\n",
      "54000\n",
      "54500\n",
      "55000\n",
      "55500\n",
      "56000\n",
      "56500\n",
      "57000\n",
      "57500\n",
      "58000\n",
      "58500\n",
      "59000\n",
      "59500\n",
      "60000\n",
      "60500\n",
      "61000\n",
      "61500\n",
      "62000\n",
      "62500\n",
      "63000\n",
      "63500\n",
      "64000\n",
      "64500\n",
      "65000\n",
      "65500\n",
      "66000\n",
      "66500\n",
      "67000\n",
      "67500\n",
      "68000\n",
      "68500\n",
      "69000\n",
      "69500\n",
      "70000\n",
      "70500\n",
      "71000\n",
      "71500\n",
      "72000\n",
      "72500\n",
      "73000\n",
      "73500\n",
      "74000\n",
      "74500\n",
      "75000\n",
      "75500\n",
      "76000\n",
      "76500\n",
      "77000\n",
      "77500\n",
      "78000\n",
      "78500\n",
      "79000\n",
      "79500\n",
      "80000\n",
      "80500\n",
      "81000\n",
      "81500\n",
      "82000\n",
      "82500\n",
      "83000\n",
      "83500\n",
      "84000\n",
      "84500\n",
      "85000\n",
      "85500\n",
      "86000\n",
      "86500\n",
      "87000\n",
      "87500\n",
      "88000\n",
      "88500\n",
      "89000\n",
      "89500\n",
      "90000\n",
      "90500\n",
      "91000\n",
      "91500\n",
      "92000\n",
      "92500\n",
      "93000\n",
      "93500\n",
      "94000\n",
      "94500\n",
      "95000\n",
      "95500\n",
      "96000\n",
      "96500\n",
      "97000\n",
      "97500\n",
      "98000\n",
      "98500\n",
      "99000\n",
      "99500\n",
      "100000\n",
      "100500\n",
      "101000\n",
      "101500\n",
      "102000\n",
      "102500\n",
      "106500\n",
      "107000\n",
      "107500\n",
      "108000\n",
      "108500\n",
      "109000\n",
      "109500\n",
      "110000\n",
      "110500\n",
      "111000\n",
      "111500\n",
      "112000\n",
      "112500\n",
      "113000\n",
      "113500\n",
      "114000\n",
      "114500\n",
      "115000\n",
      "115500\n",
      "116000\n",
      "116500\n",
      "117000\n"
     ]
    }
   ],
   "source": [
    "# preprocess the images and save in .npz format\n",
    "# resize -> mean = 0 -> std = 1\n",
    "# [TODO] edit the saving path as needed\n",
    "\n",
    "INPUT_SHAPE = (224, 224, 3)\n",
    "INPUT_IMAGE_MEAN = [0.485, 0.456, 0.406]\n",
    "INPUT_IMAGE_STD = [0.229, 0.224, 0.225]\n",
    "\n",
    "counter = 0\n",
    "for image, label in train_dataset:\n",
    "    image=image.asnumpy()\n",
    "    image = image.astype(np.float32)\n",
    "    image /= 255.0\n",
    "    image = (image - INPUT_IMAGE_MEAN) / INPUT_IMAGE_STD\n",
    "    image = resize(image, INPUT_SHAPE)\n",
    "    image = image.astype(np.float32)\n",
    "    np.savez(SAVING_PATH + \"train_formatted_normalized_npz/\" + str(counter), image = image)\n",
    "    counter += 1\n",
    "    if counter%500 == 0:\n",
    "        print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Savethe groundtruth matrix\n",
    "labels_df.reset_index(level=0,inplace=True)\n",
    "labels_df.to_csv(SAVING_PATH + '/binarized_labels_normalized.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating ratios and splits at the same time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the artificial missing labels \n",
    "def hide_labels_per_sample(train_labels,ratio_of_hidden_samples = 0.4):\n",
    "    number_of_labels_to_hide_per_sample = np.round(np.sum(train_labels,axis=1)*ratio_of_hidden_samples)\n",
    "    train_negative_weights = np.zeros_like(train_labels) + 1 \n",
    "    labels_with_missing_positives = np.copy(train_labels)\n",
    "    for idx, sample in enumerate(labels_array):\n",
    "        indices_to_hide = random.sample(list(np.nonzero(sample)[0]),int(number_of_labels_to_hide_per_sample[idx]))\n",
    "        labels_with_missing_positives[idx][indices_to_hide] = 0 \n",
    "        train_negative_weights[idx][indices_to_hide] = 0 \n",
    "    return labels_with_missing_positives, train_negative_weights\n",
    "\n",
    "def get_positive_weights(train_labels, pos_weights = 1):\n",
    "    train_positive_weights = np.zeros_like(train_labels) + pos_weights # We make positive weight 5 becuase of data imbalance\n",
    "    return train_positive_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work only on images with 4 or more labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# For each split, create missing labels of ratios: 0.0, 0.25, 0.5, 0.75\n",
    "SAVE_DIRECTORY = '/srv/workspace/research/mlml/Sample-level-weighted-loss/labels_balanced_4labels/'\n",
    "global_labels = pd.read_csv('/srv/workspace/research/mlml/datasets/mscoco/binarized_labels_normalized.csv')\n",
    "global_labels.drop([\"Unnamed: 0\", \"index\"],axis=1, inplace= True)\n",
    "\n",
    "#select only images with more than 4 labels\n",
    "global_labels = global_labels[global_labels.sum(axis=1) >= 4]\n",
    "labels_array = global_labels.values\n",
    "\n",
    "#making splits\n",
    "train_indices = np.asarray([global_labels.index.values]*2).T # duplicating the column because iterative splits expects >1 dim\n",
    "# create two splits\n",
    "X_half1, y_half1, X_half2, y_half2 = iterative_train_test_split(train_indices, labels_array, test_size = 0.5)\n",
    "# split each split to two splits\n",
    "X_quarter1, y_quarter1, X_quarter2, y_quarter2 = iterative_train_test_split(X_half1, y_half1, test_size = 0.5)\n",
    "X_quarter3, y_quarter3, X_quarter4, y_quarter4 = iterative_train_test_split(X_half2, y_half2, test_size = 0.5)      \n",
    "split1 = pd.DataFrame(y_quarter1, columns=global_labels.columns, index = X_quarter1[:,0])\n",
    "split2 = pd.DataFrame(y_quarter2, columns=global_labels.columns, index = X_quarter2[:,0])\n",
    "split3 = pd.DataFrame(y_quarter3, columns=global_labels.columns, index = X_quarter3[:,0])\n",
    "split4 = pd.DataFrame(y_quarter4, columns=global_labels.columns, index = X_quarter4[:,0])\n",
    "train1 = pd.concat([split1,split2,split3])\n",
    "test1 = split4\n",
    "train2 = pd.concat([split1,split2,split4])\n",
    "test2 = split3\n",
    "train3 = pd.concat([split1,split3,split4])\n",
    "test3 = split2\n",
    "train4 = pd.concat([split2,split3,split4])\n",
    "test4 = split1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "# Create the artificial labels\n",
    "ratios_to_hide = np.arange(0,1,0.25)\n",
    "for ratio in ratios_to_hide:\n",
    "    ratio_save_dir = os.path.join(SAVE_DIRECTORY,'missing_labels'+str(round(ratio, 1)))\n",
    "    os.makedirs(ratio_save_dir,exist_ok=True)\n",
    "    # Split1\n",
    "    labels_array = train1.values\n",
    "    labels_with_missing_positives, train_negative_weights = hide_labels_per_sample(labels_array,ratio)\n",
    "    labels_with_missing_positives_df = train1.copy() \n",
    "    labels_with_missing_positives_df.iloc[:,:] = labels_with_missing_positives\n",
    "    labels_with_missing_positives_df.to_csv(os.path.join(ratio_save_dir,\n",
    "                                                        'train1_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_negative_weights_df = train1.copy()\n",
    "    train_negative_weights_df.iloc[:,:]=train_negative_weights\n",
    "    train_negative_weights_df.to_csv(os.path.join(ratio_save_dir,'negative_weights1_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_positive_weights = get_positive_weights(labels_with_missing_positives)\n",
    "    train_positive_weights_df = train1.copy()\n",
    "    train_positive_weights_df.iloc[:,:]=train_positive_weights\n",
    "    train_positive_weights_df.to_csv(os.path.join(ratio_save_dir,'positive_weights'+str(round(ratio, 1))+'.csv'),\n",
    "                                                        index = False)\n",
    "    \n",
    "    test1.to_csv(os.path.join(ratio_save_dir,'test1_'+str(round(ratio, 1))+'.csv'))  \n",
    "    \n",
    "    \n",
    "    # Split2\n",
    "    labels_array = train2.values\n",
    "    labels_with_missing_positives, train_negative_weights = hide_labels_per_sample(labels_array,ratio)\n",
    "    labels_with_missing_positives_df = train2.copy() \n",
    "    labels_with_missing_positives_df.iloc[:,:] = labels_with_missing_positives\n",
    "    labels_with_missing_positives_df.to_csv(os.path.join(ratio_save_dir,\n",
    "                                                        'train2_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_negative_weights_df = train2.copy()\n",
    "    train_negative_weights_df.iloc[:,:]=train_negative_weights\n",
    "    train_negative_weights_df.to_csv(os.path.join(ratio_save_dir,'negative_weights2_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_positive_weights = get_positive_weights(labels_with_missing_positives)\n",
    "    train_positive_weights_df = train2.copy()\n",
    "    train_positive_weights_df.iloc[:,:]=train_positive_weights\n",
    "    train_positive_weights_df.to_csv(os.path.join(ratio_save_dir,'positive_weights'+str(round(ratio, 1))+'.csv'),\n",
    "                                                        index = False)\n",
    "    \n",
    "    test2.to_csv(os.path.join(ratio_save_dir,'test2_'+str(round(ratio, 1))+'.csv'))  \n",
    "    \n",
    "    # Split3\n",
    "    labels_array = train3.values\n",
    "    labels_with_missing_positives, train_negative_weights = hide_labels_per_sample(labels_array,ratio)\n",
    "    labels_with_missing_positives_df = train3.copy() \n",
    "    labels_with_missing_positives_df.iloc[:,:] = labels_with_missing_positives\n",
    "    labels_with_missing_positives_df.to_csv(os.path.join(ratio_save_dir,\n",
    "                                                        'train3_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_negative_weights_df = train3.copy()\n",
    "    train_negative_weights_df.iloc[:,:]=train_negative_weights\n",
    "    train_negative_weights_df.to_csv(os.path.join(ratio_save_dir,'negative_weights3_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_positive_weights = get_positive_weights(labels_with_missing_positives)\n",
    "    train_positive_weights_df = train3.copy()\n",
    "    train_positive_weights_df.iloc[:,:]=train_positive_weights\n",
    "    train_positive_weights_df.to_csv(os.path.join(ratio_save_dir,'positive_weights'+str(round(ratio, 1))+'.csv'),\n",
    "                                                        index = False)\n",
    "    \n",
    "    test3.to_csv(os.path.join(ratio_save_dir,'test3_'+str(round(ratio, 1))+'.csv'))  \n",
    "\n",
    "                                  \n",
    "    # Split4\n",
    "    labels_array = train4.values\n",
    "    labels_with_missing_positives, train_negative_weights = hide_labels_per_sample(labels_array,ratio)\n",
    "    labels_with_missing_positives_df = train4.copy() \n",
    "    labels_with_missing_positives_df.iloc[:,:] = labels_with_missing_positives\n",
    "    labels_with_missing_positives_df.to_csv(os.path.join(ratio_save_dir,\n",
    "                                                        'train4_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_negative_weights_df = train4.copy()\n",
    "    train_negative_weights_df.iloc[:,:]=train_negative_weights\n",
    "    train_negative_weights_df.to_csv(os.path.join(ratio_save_dir,'negative_weights4_'+str(round(ratio, 1))+'.csv'))\n",
    "    \n",
    "    train_positive_weights = get_positive_weights(labels_with_missing_positives)\n",
    "    train_positive_weights_df = train4.copy()\n",
    "    train_positive_weights_df.iloc[:,:]=train_positive_weights\n",
    "    train_positive_weights_df.to_csv(os.path.join(ratio_save_dir,'positive_weights'+str(round(ratio, 1))+'.csv'),\n",
    "                                                        index = False)\n",
    "    \n",
    "    test4.to_csv(os.path.join(ratio_save_dir,'test4_'+str(round(ratio, 1))+'.csv'))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating correlation weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_weights = np.zeros([len(hot_encoded), len(LABELS_LIST)])\n",
    "for sample_idx in range(len(hot_encoded)):\n",
    "    for label_idx in range(len(LABELS_LIST)):\n",
    "        if hot_encoded.iloc[sample_idx, label_idx+1] == 1:\n",
    "            negative_weights[sample_idx, label_idx] = 0\n",
    "        else:\n",
    "            temp_combination = hot_encoded.iloc[sample_idx,1:].copy()\n",
    "            temp_combination[label_idx] = 1\n",
    "            # Compare only columns that are equal to 1, and count number of matches\n",
    "            # adding one to skip the song_id column, which exists in the hot_encoded dataframe\n",
    "            positive_columns = np.where(temp_combination.values == 1)[0] + 1\n",
    "            positive_samples = len(hot_encoded[(hot_encoded.iloc[:, positive_columns].values == 1).all(axis = 1)])\n",
    "            # Count occurances with the negative sample\n",
    "            temp_combination[label_idx] = 0\n",
    "            positive_columns = np.where(temp_combination.values == 1)[0] + 1\n",
    "            total_occurances_of_pattern = len(hot_encoded[(hot_encoded.iloc[:, positive_columns].values == 1).all(axis = 1)])\n",
    "            negative_weights[sample_idx, label_idx] = (total_occurances_of_pattern - positive_samples) / total_occurances_of_pattern\n",
    "negative_weights_df = pd.DataFrame(negative_weights, columns=LABELS_LIST)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the weights\n",
    "negative_weights_df.index = global_labels.index\n",
    "negative_weights_df.to_csv(os.path.join('/srv/workspace/research/mlml/Sample-level-weighted-loss/labels_balanced_4labels/','negative_weights_global.csv'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
